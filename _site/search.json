[
  
    {
      "title"    : "Demystifying Algorithms:&lt;br&gt;Bayes&#39; Theorem and Naïve Bayes - Part I",
      "article"  : "<div class='article col col-12 animate'> <div class='article__inner'> <div class='article__content'> <h2 class='article__title'> <a href='/Bayes-Theorem'>Demystifying Algorithms:&lt;br&gt;Bayes&#39; Theorem and Naïve Bayes - Part I<i class='ion ion-md-arrow-round-forward'></i></a> </h2> <div class='article__meta'> <time class='article__date' datetime='2023-12-29T00:00:00-05:00'>29 December 2023</time> – <span class='article__minutes'>11min read</span> </div><p class='article__excerpt'>Explaining one of the cornerstones of probability theory. A critical precursor to complex probabilistic models.</p><div class='article__bottom'><div class='article-tags__box'><a href='/tag/ai' class='article__tag'>ai</a><a href='/tag/probability' class='article__tag'>probability</a></div></div></div></div></div>",
      "category" : "",
      "tags"     : "ai and probability",
      "url"      : "/Bayes-Theorem",
      "date"     : "2023-12-29 00:00:00 -0500",
      "content"  : "Get the Fundamentals DownShort-Term PlanAs I touched on in my previous post discussing the differences between AI and machine learning, we’ve recently seen a surge in the complexity and pervasiveness of large language models (LLMs) like OpenAI’s ChatGPT and Google’s Bard. While these deep learning models deservedly receive much of the spotlight of public attention as a result of their complexity and their ability to create human-like text on demand, I believe it is important to recognize some of the underlying elements and algorithms that make these state of the art models possible.I think what I would like to do, at least for a few posts (who knows, maybe more!), is to introduce some foundational algorithms and models in machine learning, many of which form the underpinnings of the more advanced, popular AI models. I’d like to cover why they are important and relevant, their common use-cases, and I’d like to step through their inner-workings in relatively plain language.Bayesian FusionWhen you start to pick apart the world of machine learning and AI in general, it becomes clear that, similar to the fields of mathematics or chemistry, modern achievements are built upon the work of those that came before. We can’t begin to build predictive and generative models without first understanding foundational elements of probability. Because without estimating the probability of an event, how would you ever hope to predict it or generate outcomes based on it?One of these foundational elements of machine learning, and what I’d like to delve into today, is Bayes’ Theorem and its derivative, the Naïve Bayes algorithm. These pieces of machine learning application help bridge the gap between mathematical theory and practical applications in AI. Originating from the work of its namesake, 18th-century mathematician, philosopher, and minister, Thomas Bayes, Bayes’ Theorem creates a framework for relating probabilities and estimating the change in probability of an event, given new observed evidence. [1] In essence, it allows you to update beliefs, in light of new information. It is a component under the hood of many of today’s machine learning-based tools. Common applications include email/text spam filtering, medical diagnostic systems, search engine recommendations, and more. Pivoting from the world of these and more advanced models, let’s dive into Bayes’ Theorem and Naïve Bayes models to illustrate how relatively simple concepts can be extended out into complex systems to empower and elevate artificial intelligence.          Reverend Thomas BayesUnderstanding Bayes’ TheoremA Palatable ExampleImagine that you are a person who, for some time, have heard and lived by the advice that all carbohydrates are detrimental to your health. You read a magazine that anyone concerned about their health should go out of their way to avoid carbs at all costs, be it crackers, cereals, bread, or sweets. After a while of eating nothing only fats and protein, you begin to feel lethargic and easily drained. You get headaches and can’t seem to think clearly. At the suggestion of a friend, you make an appointment with a nutritionist. During this consultation, this nutritionist tells you that actually, carbs aren’t such a bad thing. Sure, you should balance them with the other macronutrients and make sure you’re prioritizing slower-digesting starches over sucrose and fructose in sweets, but she tells you that your brain actually needs a minimal amount of carbohydrates to function properly! So what should you do?This situation exemplifies a case where your beliefs are directly contradicted by new information. It is highly likely that revising your dietary opinions based on a licensed nutritionist’s recommendation, rather than what a sensationalized magazine article suggests, would be more beneficial. To make an informed decision about your diet, you need to update your belief system, taking into consideration this advice. This process of revising beliefs in the light of new, credible evidence is a practical application of Bayes’ Theorem, where prior beliefs are updated with new data to form a more accurate understanding or hypothesis.Practical Application - Another Healthy DoseSo, we can think of a situation where one might have to change their beliefs due to specific circumstances, how is this actually applied in probability theory? Bayes’ Theorem can be a little difficult to wrap your brain around for new learners, but once you crack it, it becomes clear how elegant it actually is. I’ll ease you into it with another example.Say that there is a rare disease, Boogie Fever that 2% of the human population carries a hereditary gene for. So, out of every one hundred people, two of them will develop Boogie Fever at some point in their lifetime. There is a test for this gene that is pretty good but isn’t perfect, like all medical tests. It correctly identifies people with the Boogie Fever gene 95% of the time for people who actually have it (sensitivity of 95%), and it will correctly show a negative result 90% of the time for people who don’t have it (specificity of 90%). Let’s show the information we know so far:Gene Prevalence (Prior Probability): 2%. We know that on average, 2 in 100 people actually have the gene for Boogie Fever.Test Sensitivity (True Positive Rate): 95%. The test will correctly identify the Boogie Fever gene in 95% of those who have it.Test Specificity (True Negative Rate): 90%. The test will correctly show a negative result for 90% of people who don’t carry the gene.Now, suppose that Bob walks into the Midnight Special’s Center for Boogie Fever Research and gets tested. Bob gets a positive result in the mail two weeks later, telling him that he’s likely to carry the Boogie Fever gene. But just how likely? What is the probability that Bob carries the gene, given his positive test result?This is where Bayes’ Theorem allows us to do some probabilistic magic. Here is the formula:\[P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\]This formula should give you an idea of the kind of conditional probability inferences that we can make using the concept represented by Bayes’ Theorem. British geophysicist and significant contributor to the field of probability theory, Sir Harold Jeffreys, described the theorem as “to the theory of probability what Pythagoras’s theorem is to geometry”.[2] Just as the Pythagorean Theorem is used to infer previously unknown relationships in geometry, Bayes’ Theorem lets you infer new probabilities using relationships.  “[Bayes’ Theorem] is to the theory of probability what the Pythagorean theorem is to geometry.”  Sir Harold JeffreysGiven the information we have about Bob’s situation, we can apply the theorem with a little added complexity. This additional complexity is due to the fact that we’re not just interested in the direct relationship between having the Boogie Fever gene and testing positive. Instead, we want to know the probability of having the gene, given a positive test result. This is a more complex scenario because we have to consider two possibilities for a positive test result, a true positive and false positive. I won’t go into depth on the additions for accounting for false results, but this is the extended generic equation:\[P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|\neg A) \cdot P(\neg A)}\]This new equation, taking into account the accuracy of the test, prevalence of the gene, and two cases for positive and negative results respectively, provides a more accurate assessment of the likelihood of having the gene in the context of real-world testing scenarios. Plugging our scenario in looks like this:\[P(\text{Gene} | \text{Positive Test}) = \frac{P(\text{Positive Test} | \text{Gene}) \cdot P(\text{Gene})}{P(\text{Positive Test} | \text{Gene}) \cdot P(\text{Gene}) + P(\text{Positive Test} | \text{No Gene}) \cdot P(\text{No Gene})}\]And thus, adding in our probabilities provides us with the answer:\[P(\text{Gene} | \text{Positive Test}) = \frac{ 0.95 \cdot 0.02 }{ 0.95 \cdot 0.02 + 0.10 \cdot 0.98} \approx 0.16239\]Bob might receive his test result and be struck by the thought that he’s doomed to develop Boogie Fever. But with a little probability inference, we have calculated that despite his positive result, there’s only a roughly 16% chance that he actually carries the gene! So, not everything is as clear cut as it initially seems. This example illustrates just a single circumstance where Bayes’ theorem is useful in estimating a conditional probability from related probabilities. It also underscores the balancing act between sensitivity and specificity in medical diagnostic testing design. It’s very important that the test is sensitive in that it detects what you’re looking for, but it’s maybe equally important to limit the amount of false positives that come with a low specificity.But, we’re not done just yet. Let’s prove this estimate with some simulation!Simulating ProbabilitiesUsing Python, I will demonstrate the above scenario by simulating testing patients using randomly generated numbers. I won’t step through every line of the code, but in essence, it simulates 1 million patients with a gene prevalence of 2%, a test sensitivity of 95% (true positive rate), and specificity of 90% (true negative rate). Note that due to the stochastic nature of this simulation, it will result in a final probability that might differ slightly from our estimated probability of ~16.24%. However, it ought to be very close, as Bayes’ Theorem gave us the mathematical framework to properly estimate it. The code follows:# Importing numpyimport numpy as np# Parameters for the scenariopopulation = 1000000 # size of the population to simulateprevalence = 0.02 # 2% of the population has the genesensitivity = 0.95 # true positive ratespecificity = 0.90 # true negative rate# Simulating the population# True for individuals with the gene, False for those withoutgene = np.random.rand(population) &lt; prevalence# Simulating test results# True for a positive test, False for a negative testpositive_test_given_gene = np.random.rand(population) &lt; sensitivitynegative_test_given_no_gene = np.random.rand(population) &lt; specificity# True positive: gene and positive test# False positive: no gene but positive testpositive_test = (gene &amp; positive_test_given_gene) | (~gene &amp; ~negative_test_given_no_gene)# Calculating the probability of having the gene given a positive test result and outputtingprobability_gene_given_positive_test = np.mean(gene[positive_test])print(probability_gene_given_positive_test)0.16203537554473213As we can see from the output above, the simulated probability of having the Boogie Fever gene given a positive test result is ~16.2%. Aside from a discrepancy of a few hundredths of a percentage point, we are spot on! It’s clear that this is a highly performant way to update probability estimates in light of new evidence.Moving from Theorem to ModelThe practical implications of Bayes’ Theorem, seen in this example various real-world applications, underline its significance in increasingly complex and sophisticated AI models. The theorem considerably underpins the field of probability theory and it is built upon even further as we move from relatively simple probabilistic calculations to building complex predictive models. While I have yet to delve into the Naïve Bayes models, this initial discussion sets the stage for a deeper exploration in Part II! Through the process of writing this piece, I decided to break this up into two parts in order to truly dedicate enough of my discussion to the base theory before delving into the predictive power it provides under the hood of models such as Naïve Bayes. In the next post, I’ll cover how these models can lend predictive power to detecting the tone and sentiment of book reviews!Stay tuned,- JacobReferences  Wikipedia: Bayes’ Theorem  Jeffreys, Harold (1973). Scientific Inference (3rd ed.). Cambridge University Press. p. 31"
    } ,
  
    {
      "title"    : "AI vs. Machine Learning: What&#39;s the Difference?",
      "article"  : "<div class='article col col-12 animate'> <div class='article__inner'> <div class='article__content'> <h2 class='article__title'> <a href='/AI-vs-Machine-Learning'>AI vs. Machine Learning: What&#39;s the Difference?<i class='ion ion-md-arrow-round-forward'></i></a> </h2> <div class='article__meta'> <time class='article__date' datetime='2023-12-22T00:00:00-05:00'>22 December 2023</time> – <span class='article__minutes'>6min read</span> </div><p class='article__excerpt'>A brief writeup on the overlap and distinction between two of the buzziest topics of our time.</p><div class='article__bottom'><div class='article-tags__box'><a href='/tag/ai' class='article__tag'>ai</a><a href='/tag/ml' class='article__tag'>ml</a><a href='/tag/history' class='article__tag'>history</a></div></div></div></div></div>",
      "category" : "",
      "tags"     : "ai, ml, and history",
      "url"      : "/AI-vs-Machine-Learning",
      "date"     : "2023-12-22 00:00:00 -0500",
      "content"  : "Data Science Is a Confusing Word SoupI had originally planned on my first new post on this site being more of a technical walkthrough of a project that I recently completed for my Natural Language Processing (NLP) coursework. However, after reflecting on the topic and considering that NLP is nestled within the often confusing hierarchy of data science disciplines and domains, I thought it would be a good idea to talk about the “family tree” of intelligent computing disciplines and to try to illuminate some of the gray areas between these fields of study and application.It’s only quite recently that Artificial Intelligence (AI) really left the realms of science fiction and highly specialized research and entered the public zeitgeist in its genuine form. Currently, at the end of 2023, countless products and services blazon the term with seemingly every organization and individual “leveraging the power of AI”. Buzzwords in tech catch on quickly and their usage spreads exponentially, often without considerations for what the words actually mean.So What Is AI?A Bit of HistoryWhen you hear the term artificial intelligence, it probably conjures up images of KITT from Knight Rider or of a highly intelligent computer locking you out of your spacecraft to leave you to die in the cold vacuum of space. Fortunately, while sci-fi tropes shape our preconceived notions of AI, it in fact has a much more grounded history in computer science, with the term first being coined by John McCarthy of MIT.[1] McCarthy described the nascent field as “the science and engineering of making intelligent machines”.[2] AI, in its essence, is the concept of programming machines to behave like humans, accomplishing complex tasks in clever ways.An Umbrella Term TodayWhile artificial intelligence still encompasses John McCarthy’s definition, the field has been shaped and pursued due to practicality and interdisciplinary influences. McCarthy’s original vision of artificial general intelligence (AGI) is still a far-off and rather academic idea that despite what AI chatbot companies may say, does not exist today.  “I’m sorry Dave, I’m afraid I can’t do that”… yet?What AI encompasses today, is all of the methods in which we make computers “do smart stuff.” But that “stuff” is narrow in scope, i.e. Narrow AI or Weak AI. We’ve gotten very good at leveraging computing to solve specific challenges. We can create models that can detect cancer in an X-ray image, curate very personalized recommendations for what movie you should watch next, and create online chatbots that can collate information and generate sometimes alarmingly coherent and complex responses. What these examples, and much of the overall discipline of artificial intelligence in how its been shaped over the course of the last roughly 70 years, have in common is that they are all data-driven approaches. All three of these examples, and most that you would think of*, are based on collecting data and feeding it into a model, which then “learns” to produce an output.* Examples of non data-driven approaches to AI aren’t likely ones you might think of, considering AI as its commonly interpreted. They aren’t particularly “intelligent” in their own right and rely mostly on human encoding. For example, a medical diagnosis algorithm (e.g. fever, cough, and sore throat equals a common cold diagnosis) is a form of artificial intelligence as McCarthy’s era of computer scientists defined it, albeit a rudimentary one.This logically leads us to…What Is Machine Learning?OriginsMachine learning is the culmination of the early years of AI research, a long “winter” in AI breakthroughs, and both a surge in computational power and in the availability of large quantities of data.Machine learning has been around for a few decades, relegated to somewhat niche applications of advanced statistical techniques. Although simpler learning* techniques like linear regression have been around since the 18th century, it wasn’t until the rapid expansion of computing that large-scale calculations were able to be performed automatically. Such automation capability naturally led to the pursuit of more complex algorithms than your standard least-squares regression, and thus we have seen an explosion in both the number of and complexity of machine learning algorithms and models in the past two decades, particularly since the 2010s.* Machine learning essentially boils down to an algorithm processing data to “learn” numeric weights to apply to new data in order to generate an output.  All machine learning is AI, but not all AI is machine learning.Chatbots Galore!Machine learning is a data-driven way to pursue artificial intelligence, although what that means in practice is that we are limited in the intelligence we can develop by the data that we feed to any particular machine learning model. ML takes us beyond pure statistics, taking pieces from the discipline along with others, like optimization mathematics (learning weights), cognitive science (neural network modeling), obviously computer science for feasibility, implementation, and more.Even the most complex AI tools we see today, and I’m specifically talking about the large language models that are a sensation at the moment (ChatGPT, Bard, Gemini, etc.) are all very complex machine learning models. Through complicated natural language processing (NLP; subset of ML for linguistics), they are able to simulate intelligence by being fed massive amounts of textual training data. There is debate on whether these models truly “understand” language or that they are merely stochastic parrots, but that debate typically devolves into a debate on what “understand” means. And that is a digression far beyond what I want to get into!  “It’s difficult to be rigorous about whether a machine really ‘knows’, ‘thinks’, etc., because we’re hard put to define these things. We understand human mental processes only slightly better than a fish understands swimming.” [3]  John McCarthyWhat I Hope to Cover HereOn this blog, I will generally write about data science, with a specific focus on machine learning. I like digging into the practical applications of ML; finding a problem and solving it or creating a better solution than I otherwise could, with machine learning. It is a meteoric field with a breadth of applications and is continually changing our world and will continue to do so. I study it, work with it in my day job, and generally find it to be endlessly fascinating, and I hope you do too! Otherwise, you might not have read this line.Thanks for reading!- JacobReferences  Wikipedia: John McCarthy  “The Little Thoughts of Thinking Machines”, Psychology Today, December 1983, pp. 46–49.  Stanford University: Professor John McCarthy"
    } ,
  
    {
      "title"    : "Introduction",
      "article"  : "<div class='article col col-12 animate'> <div class='article__inner'> <div class='article__content'> <h2 class='article__title'> <a href='/Introduction'>Introduction<i class='ion ion-md-arrow-round-forward'></i></a> </h2> <div class='article__meta'> <time class='article__date' datetime='2023-12-17T00:00:00-05:00'>17 December 2023</time> – <span class='article__minutes'>1 min read</span> </div><p class='article__excerpt'>Introduction to this site and my intentions for it!</p><div class='article__bottom'><div class='article-tags__box'><a href='/tag/meta' class='article__tag'>meta</a></div></div></div></div></div>",
      "category" : "",
      "tags"     : "meta",
      "url"      : "/Introduction",
      "date"     : "2023-12-17 00:00:00 -0500",
      "content"  : "Gotta Start SomewhereHello and and welcome to my site! My plan is for all of this to serve as a kind of blog/portfolio/brain-dump that I hope to stick with. I discuss my intentions a bit in my About section, but it boils down to wanting to share some of the work that I do between my actual work, graduate courses, and personal reading and research. Austin Kleon says in his book, Show Your Work! that “if your work isn’t online, it doesn’t exist”.  “If your work isn’t online, it doesn’t exist.”  Austin KleonI hope to share snippets of what I know and what I’m continuing to learn within the realm of data science and beyond. And in doing so, I hope to reinforce the knowledge for myself and to improve how I communicate these concepts to a less technical audience. That audience may be no one at all, but that is beside the point. Content here will consist of short writeups on tools and workflows I find interesting, project demos, and potential deviations into whatever may be the then-current object of my fascination.Thanks for reading if you are still here! More to come."
    } ,
  
    {
      "title"    : "A Post From My Old Site",
      "article"  : "<div class='article col col-12 animate'> <div class='article__inner'> <div class='article__content'> <h2 class='article__title'> <a href='/A-Post-From-My-Old-Site'>A Post From My Old Site<i class='ion ion-md-arrow-round-forward'></i></a> </h2> <div class='article__meta'> <time class='article__date' datetime='2020-05-22T00:00:00-04:00'>22 May 2020</time> – <span class='article__minutes'>26min read</span> </div><p class='article__excerpt'>&quot;Wrangling and Regression Modeling of Scraped Zillow Listings&quot;. An example from my early learning days back in 2020, that was hosted on my deprecated site. Shows an end-to-end project with scraping web data, collating and processing it, and analyzing it for rough prediction.</p><div class='article__bottom'><div class='article-tags__box'><a href='/tag/scraping' class='article__tag'>scraping</a><a href='/tag/ml' class='article__tag'>ml</a><a href='/tag/regression' class='article__tag'>regression</a></div></div></div></div></div>",
      "category" : "",
      "tags"     : "scraping, ml, and regression",
      "url"      : "/A-Post-From-My-Old-Site",
      "date"     : "2020-05-22 00:00:00 -0400",
      "content"  : "2023 Update:This is a post that I originally published on my since-deprecated personal site in 2020. It is from about 3.5 years ago at the time of this update, before the bulk of my graduate studies and work experience. It really takes some time on the anvil to internalize and understand the important underpinning disciplines in data science. And even then, you’ll never be done! The more you know, the less you know, you know?IntroductionMy fiancée and I just purchased a home here in Bloomington, Indiana and the home buying process has inundated my thoughts lately. There’s so much to think about and do before purchasing the home itself, that the last thing that might be on your mind is whether or not you are getting the best value. Buyers are at the mercy of the market and while there is certainly room for negotiation, it’s important to know if you’re getting your money’s worth.I wanted to combine this big life event with my interest in data science, but I didn’t want to take some out-of-date and perfectly clean dataset of home sales to make some value predictions from. I wanted to take real, current data from the Bloomington housing market and see if my fiancée and I got a fair price. Rather than spending all my time working on a scraper with BeautifulSoup and HTTP, I found a prebuilt tool from ScrapeHero.com. It was actually a script on their public GitHub repo that takes a zip code and a sorting method (newest, lowest price, etc.) and give you the scraped listings as a CSV file! It isn’t perfect and the data needs some cleaning and merging to make a workable dataset, but it was perfect for getting started.The goal of this project is to take the scraped zillow data, perform any necessary data wrangling/cleaning to merge the CSVs and get them into a usable format and then build a regression model on the price and home attributes data, so that we can forecast the price of new listings.Data WranglingImportsLet’s import everything we need for this project. As with most data analysis projects using Python, we’ll need Numpy, Pandas, and our plotting library of choice. I like using Seaborn with Matplotlib so that I can use higher-level plotting functions wherever I can for simplicity. Lastly, we need a few things from Scikit-Learn for our regression analysis.import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitfrom sklearn import metricssns.set_style("whitegrid") # Set plot plot background styleReading in Scraped DataNow, let’s start working with some data. ScrapeHero’s Zillow scraper provides a simple CSV output for each zip code. Let’s read all of these into Pandas and take a look at one of them. I should note that the biggest caveat to using this script is that it is limited to one search page, so the maximum number of listings that it can pull for each zip code is about forty. This limited number of listings for the four major zip codes in Bloomington is okay for us, because it serves as a (mostly) random sample of homes for a quick analysis. It’s okay for a small-scale project like this one, but if you’re trying to scale up to get more accurate insight into your local housing market, a scraping service (or pre-collected dataset) may be required.props_47401 = pd.read_csv("properties-47401.csv")props_47403 = pd.read_csv("properties-47403.csv")props_47404 = pd.read_csv("properties-47404.csv")props_47408 = pd.read_csv("properties-47408.csv")props_47401.head()                  title      address      city      state      postal_code      price      facts and features      real estate provider      url                  0      Condo for sale      NaN      Bloomington      IN      47401      $135,000      2 bds, 2.0 ba ,1232 sqft      Bloomington - WEICHERT, REALTORS - The Brawley...      https://www.zillow.com/homedetails/3856-S-Laur...              1      House for sale      NaN      Bloomington      IN      47401      $630,000      5 bds, 5.0 ba ,3558 sqft      FC Tucker/Bloomington REALTORS      https://www.zillow.com/homedetails/1480-E-Sout...              2      House for sale      NaN      Bloomington      IN      47401      $324,900      3 bds, 2.0 ba ,2166 sqft      Evergreen Real Estate &amp; Auctions      https://www.zillow.com/homedetails/4049-E-Benn...              3      House for sale      NaN      Bloomington      IN      47401      $349,500      4 bds, 2.0 ba ,2670 sqft      FC Tucker/Bloomington REALTORS      https://www.zillow.com/homedetails/9430-E-Cona...              4      For sale by owner      NaN      Bloomington      IN      47401      $239,000      3 bds, 3.0 ba ,1603 sqft      NaN      https://www.zillow.com/homedetails/1993-E-Dutc...      We can see that we’re going to have to do some data wrangling in order to work with these files. We have some NaN values for missing data, home types that we may not want to include in our analysis, strings in numeric fields, as well as a compound field (facts and features) that we’ll want to break up.Merging DataframesLet’s merge these dataframes together!df = props_47401.append([props_47403, props_47404, props_47408])df.count()    title                   160    address                   0    city                    160    state                   160    postal_code             160    price                   151    facts and features      160    real estate provider    139    url                     160    dtype: int64Easy-peasy. We can see from the count method above that we have 160 listings, but some fields have null values, which may be an issue. We’re really only going to need the price field and the facts and features field for our analysis, so let’s work on dropping the NaN values for price. The facts and features field has the full count of 160.Dropping NaNsHere we’re filtering the price field for any instance of a NaN value and dropping that row from the dataset. We can’t create a model from incomplete training data.df = df.dropna(subset=['price'])We’re also going to drop any rows that don’t include ‘House’ in the title field. This field includes other property types such as multi-family homes and empty lots, which are not relevant to our analysis.props = df[df["title"].str.contains('House')]props.count()    title                   76    address                  0    city                    76    state                   76    postal_code             76    price                   76    facts and features      76    real estate provider    76    url                     76    dtype: int64We can see that this severely cuts our dataset down. However, a sample size of 76 is still fine and should give us reliable results given the assumptions of the methods we’ll use.Converting Price String to FloatOur target field is price, so we need to make sure that the data in it is usable. The scraper populates this field with a string value including the dollar sign. We’ll have to get rid of that and convert the field to a numeric type.We’ll do some regex magic to drop the dollar sign from all the price values. In the same line, we’re converting the type to float, so that we can apply mathematical functions to the field.prices = props['price'].replace( '[\$,)]', '', regex=True ).astype(float)prices.median()    289450.0prices.describe()    count        76.000000    mean     327928.947368    std      161696.034019    min       29900.000000    25%      218675.000000    50%      289450.000000    75%      401725.000000    max      995000.000000    Name: price, dtype: float64We have some of our first statistics! The describe method above shows us some of the characteristics of the price data. We have a sample median of $289,450, which is much higher than the state average of $158,690 (Zillow), but that’s Bloomington. This median is lower than the mean, which suggests that the distribution of prices is positively skewed or that we have some outliers pushing the average up. We’ll find out shortly.Distribution of Home PricesLet’s take a closer look at the distribution of our listing prices. We can see from the histogram/density plot below that the distribution is near-normal with a positive skew. There is a dense clustering of listing prices in the low $200k range. It’s important to note that we have some outliers that may impact our analysis.plt.figure(figsize=(8,5))ax = sns.distplot(prices, bins=50, rug=True)In the box plot below, we can see some of the outliers I mentioned. There is a value at or near $800,000 and one near $1,000,000 that are well above the rest of the distribution. At the bottom end of the distribution there is also a value near $30,000 that may affect the analysis results as well. It is oftentimes not a good idea to remove outlier just because they represent extreme values, but we’ll have to make a decision about them later.plt.figure(figsize=(8,3))ax = sns.boxplot(prices, width=0.66)Splitting Out AttributesNow, we need to obtain our feature fields, which are going to be number of bedrooms, number of bathrooms, and the square footage for each listing. These, however, are combined in the facts and features field, so we need to split them out into new columns.We do that below using the pd.str.split() method to break the values up by comma. Luckily, all the values in this field are consistent, so we don’t have to do much. We also concatenate this new dataframe of the three features with the price column, to create a new dataframe, props.props = pd.concat([props['price'], props['facts and features'].str.split(',', expand=True)], axis=1)Here, we simply rename the columns to reflect the values.props.columns = ['price', 'beds', 'baths', 'sqft']props.head()                  price      beds      baths      sqft                  1      $630,000      5 bds      5.0 ba      3558 sqft              2      $324,900      3 bds      2.0 ba      2166 sqft              3      $349,500      4 bds      2.0 ba      2670 sqft              6      $195,000      3 bds      2.0 ba      1800 sqft              10      $614,900      5 bds      5.0 ba      4170 sqft      Looks good!Type ConversionOne of the final steps we need to take before we can analyze our data is to convert all the fields to proper numeric types. Since our target, price, and our three feature fields are all quantitative, we’ll use floats. We do have to work some more regex magic to do some tricky character filtering.props['price'] = props['price'].str.replace(r'\D', '').astype(float) # removes all non-numeric charactersprops['beds'] = props['beds'].str.replace(r'[a-z]+', '').astype(float) # removes all letters, we want to keep the decimalsprops['baths'] = props['baths'].str.replace(r'[a-z]+', '').astype(float)props['sqft'] = props['sqft'].str.replace(r'[a-z]+', '').astype(float)props.head()                  price      beds      baths      sqft                  1      630000.0      5.0      5.0      3558.0              2      324900.0      3.0      2.0      2166.0              3      349500.0      4.0      2.0      2670.0              6      195000.0      3.0      2.0      1800.0              10      614900.0      5.0      5.0      4170.0      We’re finally done with cleaning and (almost) ready to move on to some analysis! We need to make a decision about those outliers first.Resolving OutliersThe three outliers that we mentioned above might give us some trouble. Like I stated before, for statistical integrity, you don’t want to go removing every outlier value that won’t give you the results you want. In this case, I think there are reasons to remove them. First, I think the low data point can be chalked up as a sampling error. This property is listed at $29,900, which is insanely low for any liveable home on a plot of land. It is likely a foreclosure auction and that is the starting bid.The other two are trickier to make a decision on, but at $800,000+, I think it’s safe to say that they are not worth considering for a representation of the homes we’re looking for in Bloomington. One could run the regression model with or without them and results may vary. Below, we use the df.loc() method to extract all the rows with price values within a range to remove the outliers and show the distribution with a box plot.props = props.loc[(props['price'] &gt; 100000) &amp; (props['price'] &lt; 700000)]plt.figure(figsize=(8,3))ax = sns.boxplot(props['price'], width=0.66)AnalysisWe’re ready to start looking at how our other attributes might be affecting the listing price for each home. We want to examine any relationship between two variables to see if there is a correlation. For example, if square footage increases, does price increase? We can visualize these relationships be creating a scatter plot for two variables at a time. Let’s create a scatter matrix so that we can do this for all possible relationships. It will also show us the frequencies of values for each field.sns.set_style("white")ax = pd.plotting.scatter_matrix(props, alpha=0.66, figsize=(10, 10))plt.show()That’s a lot to look at! But we can see that the relationships appear to be linear. This makes sense, as you would expect a house with low square footage to have fewer bedrooms, fewer bathrooms, and a generally lower price than a larger home with more amenities. Linearity is also an important assumption for the regression analysis that we’ll be performing on the data. However, we need to do more than assume linearity from scatter plots.CorrelationLet’s try to quantify these relationships. The best way that we can do this, since all the fields are numeric and nominal, is to use Pearson’s correlation coefficient. We do that below with df.corr().props.corr(method='pearson')                  price      beds      baths      sqft                  price      1.000000      0.617851      0.767242      0.828082              beds      0.617851      1.000000      0.631199      0.658591              baths      0.767242      0.631199      1.000000      0.794933              sqft      0.828082      0.658591      0.794933      1.000000      We can see above that all the relationships between two variables are positive, with some very strong correlation in some cases. A stronger linear relationship is quantified with a number closer to 1, where no relationship is defined as 0. For example, the strongest relationship here is between square footage and price, with a value of 0.828. The lowest is between bedrooms and price, which isn’t surprising, as bedrooms generally follow square footage and bathrooms in pricing guidelines for a home. We’re all set to run our regression to see if we can make predictions from our data.Multiple Linear RegressionSince we have one target field, price, and three attribute fields, we’re going to be using a multiple linear regression model to incorporate all three feature fields in predicting a price value. We’ll define our x (features) and y (target) variables below.y = props['price'].values # target - must reshape series objectx = props[['beds', 'baths', 'sqft']] # featuresNext, we’re going to split out our training and testing data. This is optional depending on how you want to use your model. I’m not going to scrape new data or generate more than one other listing (besides my home) to test the model against. So I’m taking the dataset and splitting it into 80% training data to train the regression model on, and 20% testing data to test its accuracy in predicting listing prices.x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)Alright, with that out of the way, it’s time to fit the regression model to the data. Below we are setting the ‘regressor’ variable as an instance of the LinearRegression class from Scikit-Learn. This allows us to call the .fit() method to fit the model to our variables.regressor = LinearRegression()regressor.fit(x_train, y_train)    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)The output above tells us that it worked with no problems! Now we can look at the results of the regression! We’ll first view the intercept and coefficients for the regression lines below. Since this is a multiple regression with three independent variables, we’re not going to try to visualize the model (because that would require four dimensions).print(regressor.intercept_)print(regressor.coef_)    13161.326762240089    [ 8896.63089462 54118.2658778     56.54988965]These numbers would give us what we need to plot individual regression lines or planes if we decided to focus on one or two independent variables, respectively. But what is important is running the test values through the model and seeing how it performed. We’ll test that data now.y_pred = regressor.predict(x_test)Success! Let’s look at the results of the prediction against the actual data. This will give us an indication of how our model is performing. Below, we create a new dataframe and include the actual listing prices of the test data and flatten the model results into the second column.predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})predictions                  Actual      Predicted                  0      275000.0      307954.310719              1      349000.0      344711.738989              2      209000.0      281676.910253              3      132000.0      119907.586452              4      219900.0      374400.452539              5      159900.0      143884.739662              6      429900.0      457490.538650              7      250000.0      262732.675735              8      234900.0      232233.986996              9      549900.0      385502.507644              10      499900.0      487575.079942              11      239900.0      177794.720000              12      189900.0      210688.479040              13      389900.0      411703.404771              14      344900.0      398131.431255      It looks the model worked decently!There are some predictions that are very close: row 8 at $235k actual $232k predictedas well as some that are pretty far off: row 11 at $240k actual and $178k predictedI would guess that a lot of this variation is informative of housing data in general, the same house could be listed at wildly different prices based on location, assumptions of the seller, or as a result of a million different variables that aren’t included in this data, such as a recent renovation which could drive the price up by thousands. Let’s try to dig into the results a little more.PerformanceWe can visualize the house price predictions against their actual values by plotting them side-by-side on a bar graph. We’ll take a random sample of ten and plot them using matplotlib.perf = predictions.sample(10)perf.plot(kind='bar',figsize=(16,10))plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')plt.show()Not perfect by any means, but not terrible! There are many predictions that are pretty close. Number 8 stands out as almost identical, but we saw that in the table above. 9 stands out as the worst prediction, at ~$164,000 off! That’s a huge error. There must be something unique going on with that house. It could be a remodel or it has something to offer that drives up the value that isn’t included in our three attributes. This emphasizes that our data if far from ideal, and we could use some combination of more features and more samples.To get some more indication of the performance of the model, let’s get some important regression metrics below.print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))   print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))    Mean Absolute Error: 44677.85621856667    Root Mean Squared Error: 66516.42401199984Generally mean squared error is useful, but we’ll pay attention to two other metrics for this model since we can use root mean squared error in its place. The root mean squared error (RMSE) is widely used for regression metrics because it gives an indication of how errors are affecting the model. With RMSE, errors are first squared before being averaged, so this metric points out models with large errors between the actual and predicted values.The mean absolute error (MAE) is useful because it is not so harsh on model errors. MAE tells us the mean of the absolute error for all predictions in the model. This means that all predictions are weighed equally and it gives us an idea of general performance.The RSME score of 66516 tells us that due to some large errors, the model is being thrown off by a not insignificant amount. The MAE score of 44678 tells us that the mean amount that the predictions are off is ~$45,000. This is definitely more than we want to see, but there are limitations in our data and it may be performing badly because of that.My Home ValueLimitations of our data aside, I’d like to know how the model predicts my home’s value. We found it listed on Zillow for $186,000 with three bedrooms, one and half bathrooms, and it’s 1,120 square feet. Let’s make these values into an array and plug them into the model’s .predict() method to see what it predicts.new_house = np.array([3.0, 1.5, 1120])new_house = new_house.reshape(1,-1)print('Predicted Home Price: \n', regressor.predict(new_house))    Predicted Home Price:     [184364.49466694]That’s very close! The predicted value of $184,364 is only about 0.9\% lower than the actual listing price. While this individual result might be a fluke result of error variation, it means that the model is a step in the right direction. More data and more data attributes would likely provide a model with much higher accuracy.ConclusionsOverall, this was an interesting analysis and I think it leaves a lot of room to scale it up and build a more robust model. The script could be modified to automatically or at least manually scrape data for all pages in the Zillow search and increase in data points would provide a better data set to train it the model on. We fetched some raw data, cleaned and transformed it, and ran a halfway decent model on it to provide an estimate for the value of a home. In the future, I would like to scale this project up and explore different algorithmic approaches to providing a model solution. Thanks for reading!Note About Dropping OutliersImprovement in MAE of ~$20000 from dropping the three outliers.“Excluding extreme values solely due to their extremeness can distort the results of the study by removing information about the variability inherent in the study area. You’re forcing the subject area to appear less variable than it is in reality.” - Jim FrostIf I was working with a larger, more representative sample of homes in my budget range, I would not be removing outliers and threatening the integrity of the analysis. In this case, an exception was made.ResourcesScrapeHero Zillow scraping script: ScrapeHeroData: Zillow"
    } 
  
]
